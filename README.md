ğŸ§‘â€ğŸ’» Open-Source Text-Driven Talking Avatar (Prototype)
Overview

This repository contains a fully open-source prototype of a real-time talking avatar system that converts text input into a lip-synchronized talking avatar video.

The project is designed as a minimal but complete proof-of-concept for an end-to-end pipeline:

Text â†’ Speech â†’ Audio-Driven Motion â†’ Avatar Rendering â†’ Video with Audio

The focus is on temporal consistency, synchronization, modularity, and reproducibility, rather than photorealism.

Key Features

âœ… Text-to-speech (offline, no cloud APIs)

âœ… Audio-driven facial motion

âœ… Lip-sync based on speech dynamics

âœ… Head motion and blinking for realism

âœ… Combined audio + video output (single MP4)

âœ… Fully open-source dependencies

âœ… No model training required

âœ… Runs on CPU (GPU optional)

System Architecture
Text Input
   â†“
Text-to-Speech (pyttsx3)
   â†“
Speech Audio (.wav)
   â†“
Audio Feature Extraction
   â†“
Temporal Motion Model
   â†“
Avatar Renderer (2D)
   â†“
Video Frames
   â†“
Audio + Video Muxing
   â†“
Final MP4 Output


Each stage is implemented as a separate module, making the system easy to extend (e.g., neural TTS, 3D avatars, learned motion models).

Project Structure
audio_to_avatar/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ tts.wav
â”‚   â””â”€â”€ generate_audio.py
â”œâ”€â”€ motion/
â”‚   â””â”€â”€ audio_to_motion.py
â”œâ”€â”€ renderer/
â”‚   â””â”€â”€ face_renderer.py
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ final_avatar.mp4
â”œâ”€â”€ inference.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

Installation
Requirements

Python 3.9+

Windows / Linux / macOS

FFmpeg available in PATH

Install dependencies
pip install -r requirements.txt


requirements.txt

numpy
scipy
soundfile
opencv-python
moviepy
pyttsx3


No C++ compiler or GPU is required.

Usage
1ï¸âƒ£ Generate speech from text

Edit the text inside:

tts/generate_speech.py


Then run:

python tts/generate_speech.py


This creates:

audio/tts.wav

2ï¸âƒ£ Generate the talking avatar video
python inference.py


Output:

demo/final_avatar.mp4


This video contains both audio and animated avatar, fully synchronized.

Motion Model Details

The prototype uses audio-driven heuristics instead of learned models:

Energy â†’ mouth openness & jaw drop

Spectral variation â†’ lip width / rounding

Energy gradient â†’ head nodding

Independent signal â†’ eye blinking

All motion signals are temporally smoothed to prevent jitter and flicker.

This approach provides:

Stable animation

Real-time performance

Deterministic behavior

Easy extensibility

Performance

Runs at 30 FPS on CPU

Real-time capable for short utterances

Low memory footprint

GPU not required (but compatible)

Open-Source Compliance

All components are open-source:

Component	Purpose	License
pyttsx3	Text-to-Speech	MPL
NumPy	Numerical ops	BSD
SciPy	Signal processing	BSD
SoundFile	Audio I/O	BSD
OpenCV	Rendering	Apache 2.0
MoviePy	Video muxing	MIT

âŒ No proprietary models
âŒ No cloud APIs
âŒ No restricted assets

Limitations (Intentional)

This is a research prototype, not a production avatar engine.

Fixed 2D avatar

Heuristic viseme approximation

No emotion classification

No identity customization

These design choices prioritize clarity, reproducibility, and system explainability.

Extensibility

The system is intentionally modular. Possible extensions include:

Neural TTS (VITS, FastSpeech, etc.)

Phoneme-level viseme prediction

3D mesh-based avatars

Diffusion-based face rendering

GPU-accelerated inference

Multi-avatar concurrency



Demo

A short demo video is included in:

demo/final_avatar.mp4
